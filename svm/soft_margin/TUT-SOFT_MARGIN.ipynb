{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine :  Problem 2\n",
    "## Classification with Soft Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "These exercises are based on the Stanford Machine Learning Course [CS229](http://cs229.stanford.edu) of Andrew Ng. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University.\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: .[MatplotLib](https://matplotlib.org/). [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A Dataset \n",
    "\n",
    "In this exercise, you will be using support vector machines\n",
    "(SVMs) with example 2D dataset. Experimenting with these datasets\n",
    "will help you gain an intuition of how SVMs work and how to use a Gaussian\n",
    "kernel with SVMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# for plotting grayscale\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('''Read Data''')\n",
    "X = df[['x','y']].values \n",
    "y = df['label'].values\n",
    "(n,d) = X.shape\n",
    "\n",
    "m = '''Size of dataset'''\n",
    "print('There are total of {} points in dataset'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X,y):\n",
    "\n",
    "    '''Code here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft margin SVM Primal Optimization\n",
    "\n",
    "\n",
    "**Why soft margins are needed ?**\n",
    "\n",
    "Normally, in case of outliers, the model tries to Overfit and generates a so-called 'hard margins'. In this case, we can use slack variable ($\\xi_i$). The points are then allowed to have margin of slightly less than 1, but then the cost of objective function will increase by a factor of $C\\xi_i$.\n",
    "\n",
    "In this part of the exercise, you will try using different values of the $C$\n",
    "parameter with SVMs. Informally, the $C$ parameter is a positive value that\n",
    "controls the penalty for misclassified training examples. A large $C$ parameter \n",
    "tells the SVM to try to classify all the examples correctly. \n",
    "\n",
    "Given a training set $\\{(x_i, y_i) \\}$ with $1 \\leq i \\leq n, x_i \\in \\mathbb{R}^d, y_i \\in \\{+1, −1\\}$, recall that the primal\n",
    "SVM optimization problem is usually written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{w,b}\\quad & \\lVert w \\rVert^2 + C \\sum_{i=1}^n \\xi_i \\\\\n",
    "\\text{s.t.}\\quad & y_i(w^Tx_i + b) \\geq 1 - \\xi_i\\\\\n",
    "& \\xi_i \\geq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this part of the exercise, we will be using SVMs to do non-linear classification.\n",
    "In particular, you will be using SVMs with Gaussian kernels on\n",
    "datasets that are not linearly separable.\n",
    "\n",
    "<img src='./graphic/1.png' width='650' height='650'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B Gaussian Kernel\n",
    "To find non-linear decision boundaries with the SVM, we need to first implement\n",
    "a Gaussian kernel. You can think of the Gaussian kernel as a similarity function that measures the \"distance\" between a pair of examples, $(x, z)$ with $x,z\\in\\mathbb{R}^d$. The Gaussian kernel is also parameterized by a bandwidth parameter, $\\sigma$, which determines how fast the similarity metric decreases (to $0$) as the examples are further apart. \n",
    "\n",
    "The Gaussian kernel function is defined as\n",
    "\n",
    "$$\n",
    "k(x,z) = \\exp \\Big( -\\frac{\\lVert x-z\\rVert^2}{2\\sigma^2} \\Big)\n",
    "$$\n",
    "\n",
    "You should now complete the code in `gaussianKernel` to compute\n",
    "the Gaussian kernel between two examples, ($x^{(i)}$, $x^{(j)}$) simply called `x` and `z` in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianKernel(x, z, sigma):\n",
    "    '''Code here'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Kernel function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2,  1])\n",
    "z = np.array([0, 4, -1])\n",
    "\n",
    "sigma = 2\n",
    "sim = gaussianKernel(x, z, sigma)\n",
    "\n",
    "print('Value of Gaussian Kernel is {}'.format(sim))\n",
    "#this value should be about 0.325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement a vectorized version of `gaussianKernel` such that it returns the Gram matrix  (sometimes also called a \"kernel matrix\"), $K\\in \\mathbb{R}^{n\\times m}$ with\n",
    "\n",
    "$$\n",
    "K_{ij} = k(x^{(i)},z^{(j)})\n",
    "$$\n",
    "\n",
    "where $x^{(i)}$ is the $i$th row in $X$ and $z^{(j)}$ is the $j$th row in $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianKernelVec(X, Z, sigma):\n",
    "    \n",
    "    '''Code here'''\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Vector Kernel Funct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.random.rand(3,2)\n",
    "ZZ = np.random.rand(2,2)\n",
    "\n",
    "K1 = gaussianKernelVec(XX, ZZ, sigma=.5)\n",
    "\n",
    "print('Gram Matrix: {}'.format(K1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Dual Form\n",
    "\n",
    "Recall the primal support vector machine optimization problem above.\n",
    "We can form the Lagrangian:\n",
    "$$\n",
    "\\mathcal{L}(w,b,\\xi,\\alpha,\\beta) = \\frac{1}{2}w^Tw + C\\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n\\alpha_i[y_i(x_i w+b) -w +\\xi_i] - \\sum_{i=1}^n\\beta_i\\xi_i\n",
    "$$\n",
    "\n",
    "Here, the $\\alpha_i$'s and $\\beta_i$'s are our Lagrange multipliers (constrained to be $\\geq 0$).\n",
    "We won’t go through the derivation of the dual again in detail, but after\n",
    "setting the derivatives with respect to w and b to zero as before, substituting\n",
    "them back in, and simplifying, we obtain the following dual form of the\n",
    "problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{\\alpha} \\quad & \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^n y_iy_j\\alpha_i\\alpha_j \\langle x_i, x_j \\rangle \\\\\n",
    "\\text{s.t.}\\quad & 0\\leq \\alpha_i\\leq C\\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We know that $w$ can be expressed in terms of the $\\alpha_i$'s given by\n",
    "$$\n",
    "w = \\sum_{i=1}^n \\alpha_i y_i x_i,\n",
    "$$\n",
    "so that after solving the dual problem, we can use\n",
    "$$\n",
    "f(x) = w^Tx+b = \\sum_{i=1}^n \\alpha_i y_i \\langle x_i, x \\rangle +b\n",
    "$$\n",
    "to make our predictions.\n",
    "\n",
    "Since the algorithm can be written entirely in terms of the inner products\n",
    "$\\langle x, z \\rangle$, this means that we would replace all those inner products with\n",
    "$\\langle \\phi(x), \\phi(z) \\rangle$. Specificically, given a feature mapping $\\phi$, we define the corresponding\n",
    "Kernel to be\n",
    "\n",
    "$$\n",
    "k(x, z) = \\langle \\phi(x), \\phi(z) \\rangle.\n",
    "$$\n",
    "\n",
    "Then, everywhere we previously had $\\langle x, z \\rangle$ in our algorithm, we could simply\n",
    "replace it with $k(x, z)$, and our algorithm would now be learning using the\n",
    "features $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C Solving the SVM Dual Optimization problem with Quadratic Programming\n",
    "\n",
    "The SVM dual problem can be optimized by quadratic programming. You need to download and install a third party library called \"cvxopt\". It is a python library for convex optimization problems.\n",
    "\n",
    "**You may want to look into this : https://cvxopt.org/examples/tutorial/qp.html**\n",
    "\n",
    "Quadratic programming is a general optimization routine to find a minimum for a problem specified by\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_x\\quad& \\frac{1}{2} x^TPx + q^Tx\\\\\n",
    "\\text{s.t.}\\quad & Gx \\leq h\\\\\n",
    "& Ax = b\n",
    "\\end{align}\n",
    "$$\n",
    "and we have to bring the SVM dual into this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "\n",
    "def quadprog(P,q,G,h,A,b):\n",
    "    \n",
    "    sol = cvxopt.solvers.qp(cvxopt.matrix(P), \n",
    "                      cvxopt.matrix(q), \n",
    "                      cvxopt.matrix(G), \n",
    "                      cvxopt.matrix(h), \n",
    "                      cvxopt.matrix(A), \n",
    "                      cvxopt.matrix(b))\n",
    "    return np.ravel(sol['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Quadratic Program\n",
    "We write the SVM optimization problem as a Quadratic Program.\n",
    "Let $K$ be the Gram matrix with $K_{i,j} = k(x_i,x_j)$ then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P & = yy^T \\circ K\\\\\n",
    "q & = [-1, -1, -1, \\dotsc, -1] = [-1]\\\\\n",
    "G & = [-I, I]^T\\\\\n",
    "h & = [0, 1\\cdot C]^T\\\\\n",
    "A & = y\\\\\n",
    "b & = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $[1]$ is a vector with $n$ ones and $I$ is the identity matrix of size $n\\times n$.\n",
    "\n",
    "The function `lagrange_multipliers` below calculates these quantities and calls `quadprog` to obtain the $\\alpha$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrange_multipliers(X, y, C, K):\n",
    "    \n",
    "    # Compute all matrices and then call cvxopt library\n",
    "    \n",
    "    '''Code here'''\n",
    "\n",
    "    alpha = quadprog(P, q, G, h, A, b)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal bias $b$ can be determined by KKT dual-complementarity conditions which quarantees that\n",
    "$$\n",
    "y_i (w^Tx_i + b) = 1 \\quad\\text{if}\\quad 0 < \\alpha_i < C\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i (w^Tx_i + b) &= 1\\\\\n",
    "w^Tx_i + b &= y_i\\\\\n",
    "b &= y_i - w^Tx_i\\\\\n",
    "&= y_i - \\sum_{j=1}^n \\alpha_j y_j k(x_j, x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "for any $0 < \\alpha_i < C$.\n",
    "\n",
    "Implement the function `bias` below which returns $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(X, y, C, alpha, K):\n",
    "    # for stability, we use not a single alpha but all alphas which fulfill the KKT critera.\n",
    "    # And then average the result\n",
    "    \n",
    "    idx = (alpha > 1e-5) & (alpha < C)\n",
    "    beta = '''Code here'''\n",
    "    bs = '''Code here'''\n",
    "    return np.mean(bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have function to calculate $\\alpha$ and $b$. This allows us to implement the support vector machine learning routine. \n",
    "The function `svm_learn` below first computes $\\alpha$ and $b$ and then returns a function \n",
    "$$\n",
    "z\\mapsto \\sum_{i=1}^n \\alpha_i y_i k(x_i, z) +b\n",
    "$$\n",
    "which is able to make predictions for new data point $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_learn(X,y,C,kernel):\n",
    "    \n",
    "    # Kernel function is an argument here\n",
    "    K = '''Code'''                         # kernel (Gram) matrix\n",
    "    alpha = '''Code'''   # obtain lagrange multiplier \n",
    "    b = '''Code'''                 # calculate bias\n",
    "    \n",
    "    # Select support vectors.\n",
    "    sidx = alpha > 1e-5\n",
    "    SVs = X[sidx, :]    \n",
    "    \n",
    "    # SVM classification function\n",
    "    def f(Z):\n",
    "        K = kernel(SVs, Z) # compute kernel matrix\n",
    "        beta = '''Code'''\n",
    "        return '''Code'''\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision(model, points):\n",
    "    \n",
    "    '''Code here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "C = 1\n",
    "svm = svm_learn(X, y, C, lambda X, Y: gaussianKernelVec(X, Y, sigma))\n",
    "\n",
    "plot(X,y)\n",
    "plot_decision(svm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D Overfit ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try with a high value of C and see that the model tries to classify the train data too nicely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 100\n",
    "svm = svm_learn(X, y, C, lambda X, Y: gaussianKernelVec(X, Y, sigma))\n",
    "\n",
    "plot(X,y)\n",
    "plot_decision(svm, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can you further contribute to this Notebook ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
