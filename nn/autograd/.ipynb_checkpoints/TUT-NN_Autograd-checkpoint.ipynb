{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks :  Problem 3\n",
    "## Autograd. Name says it all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "Some parts of this exercises are based on the Stanford Machine Learning Course [CS229](http://cs229.stanford.edu) of Andrew Ng. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University.\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: [MatplotLib](https://matplotlib.org/) [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/).\n",
    "\n",
    "For this part, we will work with [Autograd](https://github.com/HIPS/autograd) library in Python. Imagine, you want to train a Machine Learning model. You would have to go throught the whole process of writing the Loss function forst and then computing the derivative of the function. In case of Neural Networks, this task becomes more prominent as we have to compute gradients w.r.t. all weight vectors. For a Convolutional Neural Network, the gradient computation and programming would be then an enormous task. How could we simplify it ?\n",
    "\n",
    "One one hand you can use semantic systems such as Tensorflow, or you can use Autograd library. We just have to write down the loss function using a standard numerical library like Numpy, and Autograd will give you its gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A The simple same-as-in-lecture problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lectures, we learned to compute the Backpropogation of a very simple 2 layered network (shown below). We will choose the same network to demonstrate how we can compute the gradient of the output function w.r.t. to each input variable. \n",
    "\n",
    "<img src='./graphic/2layer.png' width='450' height='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are namely 2 input variables in Layer 1, $x$ and $y$. In the second the layer the summation of these is computed and later multiplied with the third input variable $z$ to get the so-called output of the network $f$. The computation of output is modelled as a function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np   # Thinly-wrapped version of Numpy\n",
    "from autograd import grad # Gradient Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output function\n",
    "\n",
    "def f(x,y,z):\n",
    "    return (x+y)*z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now just compute the gradients of defined function by calling the **grad()** function from the library. Note that there are 3 arguments in the input function. Thus the gradient will be rather partial gradients w.r.t. each varibale. The syntax use is **grad(input_func, var_idx)**, where **var_idx** is the index of the corresponding variable. The index if 1st variable (which is x here) is **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial derivatives\n",
    "\n",
    "d_x = '''Code here''' # wrt x\n",
    "d_y = '''Code here''' # wrt y\n",
    "d_z = '''Code here''' # wrt z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivate wrt x\n",
    "print(d_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of grad() function is stored in a varibale which can compute the numerical values further. We will now define the numerical values of all the three variable and check if the results is same as it was obtained manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define values of input variables\n",
    "\n",
    "X = np.array([2])\n",
    "Y = np.array([-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z =np.array([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and check the derivate values\n",
    "\n",
    "print('The partial derivate of f wrt x = {}'.format('''Code here''')\n",
    "print('The partial derivate of f wrt y = {}'.format('''Code here''')\n",
    "print('The partial derivate of f wrt z = {}'.format('''Code here''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B Autograds with MNIST dataset\n",
    "\n",
    "In this part, we will define the Loss function for a multi layered Neural Network as seen in previous problems. The loss is function will be defined by computing a forward pass and comparing it with the true output for that input.\n",
    "\n",
    "The cost function for the neural network (without regularization) is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "where $h_\\theta \\left( x^{(i)} \\right)$ is computed as shown in the neural network figure above, and K = 10 is the total number of possible labels. Note that $h_\\theta(x^{(i)})_k = a_k^{(3)}$ is the activation (output\n",
    "value) of the $k^{th}$ output unit. Also, recall that whereas the original labels (in the variable y) were 0, 1, ..., 9, for the purpose of training a neural network, we need to encode the labels as vectors containing only values 0 or 1, so that\n",
    "\n",
    "$$ y = \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots  \\quad \\text{or} \\qquad\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ (that you should use with the cost function) should be a 10-dimensional vector with $y_5 = 1$, and the other elements equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "#  training data stored in arrays X, y\n",
    "data = loadmat('./data/ex4data1.mat')\n",
    "\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "m = y.size\n",
    "\n",
    "print('Total size of dataset is {} images'.format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with a set of network parameters ($\\Theta^{(1)}$, $\\Theta^{(2)}$) already trained by Standford Uni. They are stored in `ex4weights.mat`. Let us load those parameters into variables `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes). Note that each of the two weight matrix have one column extra.\n",
    "\n",
    "**Why ?** Remember Bias terms !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the .mat file, which returns a dictionary \n",
    "weights = loadmat('./data/ex4weights.mat')\n",
    "\n",
    "# get the model weights from the dictionary\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "print('Size of Weight-Vector for Hidden Layer is: {} '.format(Theta1.shape))\n",
    "print('Size of Weight-Vector for Output Layer is: {} '.format(Theta2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the zero digit to 0.\n",
    "# This is an artifact due to the fact that this dataset was used in...\n",
    "# ...MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# Swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# Since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Unroll parameters to inclue all information in 1 array. Two vector can be separatd later based on sizes.\n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    # Computes the sigmoid of z.\n",
    "    \n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function for 1 sample \n",
    "\n",
    "<img src='./graphic/sample.png' width='350' height='350'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Theta1,Theta2, X, y):\n",
    "    \n",
    "    '''Code here'''\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient wrt Theta1 and Theta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1_grad = grad('''Code here''') # wrt Theta1\n",
    "Theta2_grad = grad('''Code here''') # wrt Theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the loss function and gradient for each sample one by one and then average out both the value. **Why do we do this ?**\n",
    "\n",
    "**Because the the Grad function from Autograd library works with scalers and if we don't work with one-at-a-time input then the function won't simply function. There exist other functions in Autograd library: Try jacobian, elementwise_grad or holomorphic_grad for arrays.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate as zero\n",
    "J = 0\n",
    "grad_auto = np.concatenate([np.zeros(Theta1.shape).ravel(), np.zeros(Theta2.shape).ravel()])\n",
    "\n",
    "# Run loop over range of inputs\n",
    "for i in range(m):\n",
    "  '''Code here'''\n",
    "\n",
    "# Average Out\n",
    "J = J/m\n",
    "grad_auto = grad_auto/m\n",
    "\n",
    "print('Shape of concatenated gradient by Autograd is {}'.format(grad_auto.shape))\n",
    "print('First few elements look like : {}'.format(grad_auto[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute with Backpropogation\n",
    "\n",
    "**Theory:**\n",
    "Revise that the intuition behind the backpropagation algorithm. Given a training example  (ùë•(ùë°),ùë¶(ùë°)) , we will first run a ‚Äúforward pass‚Äù to compute all the activations throughout the network, including the output value of the hypothesis  ‚ÑéùúÉ(ùë•) . Then, for each node  ùëó  in layer  ùëô , we compute an ‚Äúerror term‚Äù  ùõø(ùëô)ùëó  that measures how much that node was ‚Äúresponsible‚Äù for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network‚Äôs activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$. In detail, here is the backpropagation algorithm. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "1. Perform a feedforward pass, computing the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. You have already done this part above while compute the Cost function.\n",
    "\n",
    "1. For each output unit $k$ in layer 3 (the output layer), set \n",
    "$$\\delta^{(3)} = \\left(a^{(3)} - y \\right)$$\n",
    "where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ $(y_k = 1)$, or if it belongs to a different class $(y_k = 0)$.\n",
    "\n",
    "1. For the hidden layer $l = 2$, set \n",
    "$$ \\delta^{(2)} = \\left( \\Theta^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Note that the symbol $*$ performs element wise multiplication in `numpy`.  Also you should chuck the bias term of from the Weight vector.\n",
    "\n",
    "1. Accumulate the gradient from this example using the following formula. \n",
    "$$ \\Delta^{(l)} = \\delta^{(l+1)} (a^{(l)})^{(T)} $$\n",
    "\n",
    "1. Obtain the gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(z):\n",
    "    # Grad of Sigmoid func can be written as g'(z) = g(z) * (1-g(z))\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "    g = sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "    return g\n",
    "\n",
    "def nnCostFunction(Theta1, Theta2,num_labels, X, y, lambda_=0):\n",
    "    \n",
    "    # Size of dataset\n",
    "    m = y.size\n",
    "         \n",
    "     \n",
    "    # Neural Network activations...    \n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1) # adding row of 1s\n",
    "    a2 = sigmoid(a1.dot(Theta1.T)) # first dot product and then func application  \n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1) # adding bias term again in hidden layer\n",
    "    a3 = sigmoid(a2.dot(Theta2.T))\n",
    "    \n",
    "    # Modifying Output matrix as per above explanation\n",
    "    y_matrix = y.reshape(-1)\n",
    "    y_matrix = np.eye(num_labels)[y_matrix]\n",
    " \n",
    "    # Compute J\n",
    "    J = (-1. / m) * np.sum((np.log(a3) * y_matrix) + np.log(1 - a3) * (1 - y_matrix))\n",
    "    \n",
    "    # Gradients are initiliazed\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Complete the Code here'''\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "J_n, grad_n = nnCostFunction(Theta1, Theta2,num_labels, X, y, lambda_=0)\n",
    "\n",
    "print('Shape of concatenated gradient by Backpropogation is {}'.format(grad_n.shape))\n",
    "print('First few elements look like : {}'.format(grad_n[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare two results\n",
    "\n",
    "We compute the differential term to check how similar are the two gradient vectors received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.linalg.norm(grad_n-grad_auto)/np.linalg.norm(grad_n+grad_auto)\n",
    "\n",
    "print('If your implementation is correct, then the relative difference will be small (less than 1e-9).') \n",
    "print('Relative Diff is {}'. format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can you contribute further for this notebook ?\n",
    "\n",
    "1. You may have realized that loop to compute the loss and gradient is slow. Maybe try out different function of Autograd library and find out a faster one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
