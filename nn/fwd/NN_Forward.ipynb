{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks :  Problem 1\n",
    "## Prediction by Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "These exercises are based on the Stanford Machine Learning Course [CS229](http://cs229.stanford.edu) of Andrew Ng. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University.\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: .[MatplotLib](https://matplotlib.org/). [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we work the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and we will use Neural Network to recognize hand-written digits.(from 0 to 9). Automatic digit recognition is in wide practice in the area of Banking, Postal Services and more. With this excercise the user will learn to implement the learned theory to classifiy such inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## \n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 15.10.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# for plotting grayscale\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset : \n",
    "The dataset contains 5000 training examples of handwritten digits (This is a subset of the MNIST handwritten digit dataset). As the baseline of this problem is mention before, the data set has been saved in a native Octave/MATLAB matrix format. We use the .mat format here because this is the exact same dataset provided in the MATLAB version of mentioned source assignments. The good news is that Python provides mechanism to load the dataset in given format using the **scipy.io** module. This function returns a python dictionary with keys containing the variable names within the .mat file. The dataset can be downloaded from : \n",
    "\n",
    "Simply use the below snippet of code to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = loadmat('ex3data1.mat')\n",
    "\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5000 training examples in the given dataset. Each training example is a 20 pixel by 20 pixel grayscale image. Each pixel is represented by a floating point number indicating the grayscale intensity of the pixel. The 20 by 20 grid of pixels is “flattened” into a 400-dimensional vector. As a result, each of the training examples becomes a single row in our data matrix **X**. This gives us size 5000 by 400 matrix X.\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\: (x^{(1)})^T \\: - \\\\ -\\: (x^{(2)})^T \\:- \\\\ \\vdots \\\\ - \\: (x^{(m)})^T \\:-  \\end{bmatrix} $$\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector y that contains labels for the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's a small modification to be done**. Since the original dataset was in .mat format and MATLAB doesn't have 0 indexing, the digit zero is mapped to ouput (y) = 10. Before moving further, we have to map the digit zero to output (y) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the zero digit to 0.\n",
    "# This is an artifact due to the fact that this dataset was used in...\n",
    "# ...MATLAB where there is no index 0\n",
    "y[y == 10] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A Visualize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by visualizing a subset of the training set. We plot take a random integer out of the total size of our dataset i.e. 5000 images. This index is passed to the **show_img** function to produce a grayscale image. The function works as follows: First it extract the whole row with the specified index from the X matrix. This need to be put in original shape to display correctly. Lets call this process as 'unflattening'. Here the row vector is reshaped to its original shape 20 by 20. This is done by Numpy **reshape** command. Here we use the Fortran indexing order to stack the row elements as per correct order. (Otherwise you see an incorrect image). Further, out input vector is in form of floating point numbers ranging from 0 to 1. For plotting Grayscale Image these need to be scaled to a factor of 0 to 255. \n",
    "\n",
    "Use the PIL library to plot the image. [PIL](https://pillow.readthedocs.io/en/stable/reference/Image.html)\n",
    "The image is rescaled to get better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random index\n",
    "rand_indices = np.random.choice(m, 1)\n",
    "\n",
    "def show_img(a):\n",
    "\n",
    "    # Select the row from X matrix with the specified argument\n",
    "    selc = X[a, :]\n",
    "\n",
    "    # Unflatten the row to original 20x20 size\n",
    "    selc = np.reshape(selc, (20,20), order='F')\n",
    "    \n",
    "    # Multiply by 255 to get a grayscale range in 0 to 255\n",
    "    im = Image.fromarray(selc*255)\n",
    "\n",
    "    # Increase the size for better visualization\n",
    "    size = (512,512)\n",
    "    im = im.resize(size)\n",
    "    im.show()\n",
    "\n",
    "\n",
    "show_img(rand_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B Model Representation and Feedforward Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be using a Neural Network that has already been trained. Again, the weights have been taken from the source problem of Stanford Uni. Your goal is to implement the feedforward propagation algorithm to use our weights for prediction.\n",
    "\n",
    "Out understanding of Neural Network is shown in the following figure.\n",
    "<img src='sample.png' width='350' height='350'>\n",
    "\n",
    "It has 3 layers: Input layer, Hidden layer and Output layer. Note that our inputs **X** are pixel values of digit images. Since the images are of size 20×20, this gives us 400 Input layer units. \n",
    "\n",
    "You are provided with a set of network parameters ($\\Theta^{(1)}$, $\\Theta^{(2)}$) already trained by Standford Uni. They are stored in `ex3weights.mat`. Let us load those parameters into variables `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes). Note that each of the two weight matrix have one column extra.\n",
    "\n",
    "**Why ?** Remember Bias terms !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the .mat file, which returns a dictionary \n",
    "weights = loadmat('ex3weights.mat')\n",
    "\n",
    "# get the model weights from the dictionary\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement feedforward propagation for the neural network. We will implement the feedforward computation that computes $h_\\theta(x^{(i)})$ for every example $i$ and returns the associated predictions. The prediction from the neural network will be the label that has the largest output $\\left( h_\\theta(x) \\right)_k$.\n",
    "\n",
    "**What do we do to calculate $h_\\theta(x^{(i)})$ ?**\n",
    "\n",
    "We apply a non-linear function after linear transformation. That means:\n",
    "\n",
    "$$\n",
    "h(x) = f(dot.product(W,x))\n",
    "$$\n",
    "\n",
    "where b is the Bias term.\n",
    "We have earlier seen the dimensions of our pre-given weight matrices. Note that the bias term is already included in it. We have two options: (1) We can increment the input vector X by a row of 1s or (2) We can split the computation as shown in the equation below.\n",
    "\n",
    "$$\n",
    "h(x) = f(Wx + b)\n",
    "$$\n",
    "\n",
    "We resort to the second option for this problem. Since we have a two layer Neural network, we have to apply this $h$ function twice. \n",
    "\n",
    "$$\n",
    "h_\\theta(x) = f_2(W_2f_1(W_1x + b_1) + b_2)\n",
    "$$\n",
    "\n",
    "**What is f function ?**\n",
    "For Neural Networks, there are many activations function f, as you can see in the picture shown below. But out of these a famous one is the Sigmoid Function. We use this one for our problem here. The sigmoid function computes the probabibility of each class and we can choose the output class with the max value of computed probabilities.\n",
    "<img src='2.png' width='550' height='550'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# Since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    # Computes the sigmoid of z.\n",
    "    \n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def predict(t1,t2,X):\n",
    "    # The prediction function computes the probability of every label as described in theory\n",
    "    # Find the index of the max value to give its class.\n",
    "    \n",
    "    # f1 function\n",
    "    z2 = np.dot(Theta1[:,1:],np.transpose(X).flatten())+ Theta1[:,0]\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # f2 function\n",
    "    z1 = np.dot(Theta2[:,1:],a2)+ Theta2[:,0]\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    return np.argmax(a1), a1[np.argmax(a1)]\n",
    "\n",
    "# Predict a class of a random index\n",
    "klass, probability = predict(Theta1, Theta2, X[rand_indices,:])\n",
    "print('The predicted class of the given output is {} with probability = {}'.format(klass, probability))\n",
    "print('The true class of the given output wrt dataset is {}.'.format(y[rand_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.C Predicting Accuracy of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of out predictive model, we can one by one predict all the training inputs and then compare the predicted output with the actual output.\n",
    "\n",
    "$$\n",
    "Accuracy = 100 - \\frac{err*100}{Total}\n",
    "$$\n",
    "\n",
    "where err = Count of inputs that are falsely classiifed by predictor model\n",
    "\n",
    "Total = Total number of tested inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 0 # Initiating variable\n",
    "\n",
    "# Predict for every\n",
    "for k in range(m):\n",
    "    klass_pred, pred = predict(Theta1, Theta2, X[k,:])\n",
    "    klass_act = y[k]\n",
    "    \n",
    "    # if incorrect prediction, then count as inaccuracy\n",
    "    if klass_pred != klass_act:\n",
    "        err = err + 1\n",
    "        \n",
    "\n",
    "print('Accuracy of trained model is {}%'.format((1-err/m)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
