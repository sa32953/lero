{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning :  Problem 6\n",
    "## Classification using Logistic Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Problems are slightly different that regression problems. During regression we want to redict real valued quantities of Output Varibale (y). On the other hand, in classificaion problems we want to predict discrete valued quantities of (y). \n",
    "\n",
    "### **binary classification :** $y \\in {0,1}$\n",
    "\n",
    "### **multiclass classification :** $y \\in {1,2,3,4,5,....k}$\n",
    "\n",
    "Similarly, we cannot use the Mean Square Loss function because it will give ou an output with best fit of all given datapoints, whereas out requiremement is to segregate the two (or more) types of datapoints.\n",
    "\n",
    "A number of alternative losses for classification are typically used instead. One of the most used is Logistic Loss function.\n",
    "\n",
    "### **Logistic Loss**: $L(h_w(x),y) = log( 1 + exp(-y*h_w(x)) )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first part of the problem, user needs to read the data given the '.txt' format. The given data is read with help us **pandas** command with appropritate delimiter specifications as seen in the given text file. \n",
    "\n",
    "#### See documention of the library at : https://pandas.pydata.org/\n",
    "\n",
    "The data is stores in a variable with specific name tags for each row. To segregate the data the just call by name tags and store in Input and Output varibale respectively. Simulataneously, we define the solving paramters like Learning Rate and Convergency Criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 25.09.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Dependencies imported\n",
    "\n",
    "# For vector computations and notations\n",
    "import numpy as np \n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "############################\n",
    "##       Read Data        ##\n",
    "############################\n",
    "\n",
    "# file in the same directory\n",
    "data = pd.read_csv('/home/sa0102/robot_learning/excercises/ml/log_class/log_class_data.txt', names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "\n",
    "# export pandas dataframe to numpy array\n",
    "X = data[['Exam 1','Exam 2']].values \n",
    "y = data['Admitted'].values\n",
    "\n",
    "# Regressions with logistics loss work with Classification Outputs as {-1,1}. Change the output..\n",
    "#.. label of y = 0 to y = -1\n",
    "\n",
    "for b in range(len(y)):\n",
    "    if y[b]==0: # for y = 0 labels only\n",
    "        y[b]=-1 # change to y = -1\n",
    "\n",
    "# Defining Solving Parameters\n",
    "alpha = 0.4\n",
    "acc = 10 ** -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting is done using a python library 'matplotlib'. See documention of the library at https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Visualize Data      ##\n",
    "############################\n",
    "\n",
    "# Find Indices of Positive and Negative Examples, to visualize separately\n",
    "pos = y == 1\n",
    "neg = y == -1\n",
    "\n",
    "# Plot Examples\n",
    "plt.plot(X[pos, 0], X[pos, 1], 'k*', mfc='r',ms=10)\n",
    "plt.plot(X[neg, 0], X[neg, 1], 'ko', mfc='g', ms=8)\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.legend(['Admitted', 'Rejected'])\n",
    "plt.title('University Database')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Bias Term\n",
    "\n",
    "Just as we did in LR  we do the similar procedure for adding the bias term, adding a column of **1s** to every element of input datapoint.\n",
    "\n",
    "$$\n",
    "\\phi = \\left(\\begin{array}{cc} \n",
    "X \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias\n",
    "X = np.hstack((X,np.ones((X.shape[0],1))))\n",
    "\n",
    "# Size of data\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular problem we are using the linear hypothesis function (which is dot product of weight vector and feature vector).\n",
    "\n",
    "$$\n",
    "h_w(x) = w^T.\\phi\n",
    "$$\n",
    "\n",
    "The objective of gradient descent is to minimize the Logistic loss (which was defined above):\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m}\\sum_{i=1}^m log( 1 + exp(-y_i*h_w(x_i)) )\n",
    "$$\n",
    "\n",
    "The parameters of the model are the $w_j$ values. \n",
    "These are the values that we will adjust to minimize the gradient of Loss function. This will be done using the gradient descent algorithm which performs the update:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= w_j - \\frac{\\alpha}{m} \\frac{\\partial J(w)}{\\partial w_j} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Hypothesis Func     ##\n",
    "############################\n",
    "\n",
    "def h(w,x): \n",
    "    \n",
    "    return np.dot(x,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the loss function we can implement the gradient descent routine. First, write a function $\\nabla J(w)$ which returns the gradient of the Logistic loss function. The gradient is just a vector with all the partial derivatives\n",
    "\n",
    "$$\n",
    "\\nabla J(w) = \\bigg[\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_d} \\bigg]^T\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\big( \\frac{-y_i*x_i}{1 + exp(y_i*h_w(x_i))} \\big) \n",
    "$$\n",
    "\n",
    "Below is a function with the inner part of the summation symbol. The addition for m samples can be done in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##    (Log Loss Func)     ##\n",
    "############################\n",
    "\n",
    "def log_gradient(x,w,y):\n",
    "\n",
    "    # Valid for Logarithmic Loss func only\n",
    "    # See analytical solution in slide\n",
    "\n",
    "    log_grad = -y*x/(1+ np.exp(y*h(w,x)))\n",
    "    \n",
    "    return log_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main function, user can deploy the Pseudo code as seen in lecture.\n",
    "\n",
    "You will implement gradient descent in the function `gradient_descent(x,y,w,acc,alpha)`.\n",
    "Recall the update rule of gradient descent which is\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla J(w^{(k)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent(x,y,w,acc):\n",
    "    global m, itr\n",
    "\n",
    "    delta_w = np.array([1,1,1]) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(acc < abs(a) for a in delta_w):\n",
    "        \n",
    "        log_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(m):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            log_g = log_g + log_gradient(X[a],w,y[a])/m\n",
    "            a = a+1\n",
    "        \n",
    "        # Modify weight\n",
    "        delta_w = alpha * log_g\n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "        \n",
    "    return w, itr\n",
    "\n",
    "\n",
    "# Display the obtained results\n",
    "log_weight, log_itr = gradient_decent(X,y,w,acc)\n",
    "\n",
    "print('The optimized weight vector is {}.'.format(log_weight))\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(acc,alpha))\n",
    "print('Total iterations done = {}'.format(log_itr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm will run its course and after each iteration the weight vector will be modified to get a lower Gradient of the Sqaured Loss function. Finally the weight vector is converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the classification line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the classfication line it is sufficent to just plot 2 points and connect them. We do it by finding the **max** and **min** values from each vector set of X (Input) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find min and max points\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "plot_x = np.array([x_min, x_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a classification line, we consider that the probabilty of the Output Class (y) of a given Input Point (X') is 1/2. We say that Class of a given data point is determined by \n",
    "\n",
    "$$y* = sign(h_w(X'))$$. \n",
    "\n",
    "With this consideration we can say that equation of line can be determined by follwing:\n",
    "\n",
    "$$h_w(X') = 0$$ \n",
    "\n",
    "Expanding the above equation we get:\n",
    "\n",
    "$$\n",
    "w_0* (X')_0 + w_1* (X')_1 + w_2\n",
    "$$\n",
    "\n",
    "Since we already identitifed the **max** and **min** points two plot the line, we can compute the y-axis co-ordinate by following equation:\n",
    "\n",
    "$$\n",
    "(X')_1 = \\frac{-(w_0 * (X')_1 + w_2)}{w_1} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the y axis point from the learned weights\n",
    "plot_x2 = (-1 / log_weight[1]) * (log_weight[0] * plot_x1 + log_weight[2])\n",
    "\n",
    "plt.plot(plot_x1, plot_x2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
