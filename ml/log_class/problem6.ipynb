{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning :  Problem 6\n",
    "## Classification using Logistic Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "These exercises are based on the Stanford Machine Learning Course [CS229](http://cs229.stanford.edu) of Andrew Ng. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University.\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: .[MatplotLib](https://matplotlib.org/). [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Problems are slightly different that regression problems. During regression we want to redict real valued quantities of Output Varibale (y). On the other hand, in classificaion problems we want to predict discrete valued quantities of (y). \n",
    "\n",
    "#### **binary classification :** $y \\in {0,1}$\n",
    "\n",
    "#### **multiclass classification :** $y \\in {1,2,3,4,5,....k}$\n",
    "\n",
    "Similarly, we cannot use the Mean Square Loss function because it will give ou an output with best fit of all given datapoints, whereas out requiremement is to segregate the two (or more) types of datapoints.\n",
    "\n",
    "A number of alternative losses for classification are typically used instead. One of the most used is Logistic Loss function.\n",
    "\n",
    "#### **Logistic Loss**: $L(h_w(x),y) = log( 1 + exp(-y*h_w(x)) )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.A Logistic Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first part of the problem, user needs to read the data given the '.txt' format. The given data is read with help us **pandas** command with appropritate delimiter specifications as seen in the given text file. \n",
    "\n",
    "#### See documention of the library at : https://pandas.pydata.org/\n",
    "\n",
    "The data is stores in a variable with specific name tags for each row. To segregate the data the just call by name tags and store in Input and Output varibale respectively. Simulataneously, we define the solving paramters like Learning Rate and Convergency Criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 25.09.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Dependencies imported\n",
    "\n",
    "# For vector computations and notations\n",
    "import numpy as np \n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "############################\n",
    "##       Read Data        ##\n",
    "############################\n",
    "\n",
    "# file in the same directory\n",
    "data = pd.read_csv('/home/sa0102/robot_learning/excercises/ml/log_class/log_class_data.txt', names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "\n",
    "# export pandas dataframe to numpy array\n",
    "X = data[['Exam 1','Exam 2']].values \n",
    "y = data['Admitted'].values\n",
    "\n",
    "# Regressions with logistics loss work with Classification Outputs as {-1,1}. Change the output..\n",
    "#.. label of y = 0 to y = -1\n",
    "\n",
    "for b in range(len(y)):\n",
    "    if y[b]==0: # for y = 0 labels only\n",
    "        y[b]=-1 # change to y = -1\n",
    "\n",
    "# Defining Solving Parameters\n",
    "alpha = 0.4\n",
    "acc = 10 ** -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting is done using a python library 'matplotlib'. See documention of the library at https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Visualize Data      ##\n",
    "############################\n",
    "\n",
    "# Find Indices of Positive and Negative Examples, to visualize separately\n",
    "pos = y == 1\n",
    "neg = y == -1\n",
    "\n",
    "# Plot Examples\n",
    "plt.plot(X[pos, 0], X[pos, 1], 'k*', mfc='r',ms=10)\n",
    "plt.plot(X[neg, 0], X[neg, 1], 'ko', mfc='g', ms=8)\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.legend(['Admitted', 'Rejected'])\n",
    "plt.title('University Database')\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Bias Term\n",
    "\n",
    "Just as we did in LR  we do the similar procedure for adding the bias term, adding a column of **1s** to every element of input datapoint.\n",
    "\n",
    "$$\n",
    "\\phi = \\left(\\begin{array}{cc} \n",
    "X \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias\n",
    "X = np.hstack((X,np.ones((X.shape[0],1))))\n",
    "\n",
    "# Size of data\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular problem we are using the linear hypothesis function (which is dot product of weight vector and feature vector).\n",
    "\n",
    "$$\n",
    "h_w(x) = w^T.\\phi\n",
    "$$\n",
    "\n",
    "The objective of gradient descent is to minimize the Logistic loss (which was defined above):\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m}\\sum_{i=1}^m log( 1 + exp(-y_i*h_w(x_i)) )\n",
    "$$\n",
    "\n",
    "The parameters of the model are the $w_j$ values. \n",
    "These are the values that we will adjust to minimize the gradient of Loss function. This will be done using the gradient descent algorithm which performs the update:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= w_j - \\frac{\\alpha}{m} \\frac{\\partial J(w)}{\\partial w_j} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Hypothesis Func     ##\n",
    "############################\n",
    "\n",
    "def h(w,x): \n",
    "    \n",
    "    return np.dot(x,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the loss function we can implement the gradient descent routine. First, write a function $\\nabla J(w)$ which returns the gradient of the Logistic loss function. The gradient is just a vector with all the partial derivatives\n",
    "\n",
    "$$\n",
    "\\nabla J(w) = \\bigg[\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_d} \\bigg]^T\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\big( \\frac{-y_i*x_i}{1 + exp(y_i*h_w(x_i))} \\big) \n",
    "$$\n",
    "\n",
    "Below is a function with the inner part of the summation symbol. The addition for m samples can be done in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##    (Log Loss Func)     ##\n",
    "############################\n",
    "\n",
    "def log_gradient(x,w,y):\n",
    "\n",
    "    # Valid for Logarithmic Loss func only\n",
    "    # See analytical solution in slide\n",
    "\n",
    "    log_grad = -y*x/(1+ np.exp(y*h(w,x)))\n",
    "    \n",
    "    return log_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main function, user can deploy the Pseudo code as seen in lecture.\n",
    "\n",
    "You will implement gradient descent in the function `gradient_descent(x,y,w,acc,alpha)`.\n",
    "Recall the update rule of gradient descent which is\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla J(w^{(k)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent(x,y,w,acc):\n",
    "    global m, itr\n",
    "\n",
    "    delta_w = np.array([1,1,1]) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(acc < abs(a) for a in delta_w):\n",
    "        \n",
    "        log_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(m):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            log_g = log_g + log_gradient(X[a],w,y[a])/m\n",
    "            a = a+1\n",
    "        \n",
    "        # Modify weight\n",
    "        delta_w = alpha * log_g\n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "        \n",
    "    return w, itr\n",
    "\n",
    "\n",
    "# Display the obtained results\n",
    "log_weight, log_itr = gradient_decent(X,y,w,acc)\n",
    "\n",
    "print('The optimized weight vector is {}.'.format(log_weight))\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(acc,alpha))\n",
    "print('Total iterations done = {}'.format(log_itr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm will run its course and after each iteration the weight vector will be modified to get a lower Gradient of the Sqaured Loss function. Finally the weight vector is converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the classification line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the classfication line it is sufficent to just plot 2 points and connect them. We do it by finding the **max** and **min** values from each vector set of X (Input) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find min and max points\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "plot_x = np.array([x_min, x_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a classification line, we consider that the probabilty of the Output Class (y) of a given Input Point (X') is 1/2. We say that Class of a given data point is determined by \n",
    "\n",
    "$$y* = sign(h_w(X'))$$. \n",
    "\n",
    "With this consideration we can say that equation of line can be determined by follwing:\n",
    "\n",
    "$$h_w(X') = 0$$ \n",
    "\n",
    "Expanding the above equation we get:\n",
    "\n",
    "$$\n",
    "w_0* (X')_0 + w_1* (X')_1 + w_2\n",
    "$$\n",
    "\n",
    "Since we already identitifed the **max** and **min** points two plot the line, we can compute the y-axis co-ordinate by following equation:\n",
    "\n",
    "$$\n",
    "(X')_1 = \\frac{-(w_0 * (X')_1 + w_2)}{w_1} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the y axis point from the learned weights\n",
    "plot_x2 = (-1 / log_weight[1]) * (log_weight[0] * plot_x1 + log_weight[2])\n",
    "\n",
    "plt.plot(plot_x1, plot_x2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.B Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the learned model, we can predict the class of an unknown data point. Suppose we want to predict if a student with Exam_1 score = 40 and Exam_2 score = 80 will fall under the **Admitted** or **Rejected** class. \n",
    "\n",
    "The class can be predicted by computing the sign of hypothesis function value. Ee do the similar procedure for adding the bias term, adding a column of 1s to every element of unknown input datapoint. \n",
    "\n",
    "$$\n",
    "Class = sign(h_w(x*))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##      Unknown Data      ##\n",
    "############################\n",
    "\n",
    "# Suppose the learned weight is calculated from part A of this problem\n",
    "log_weight = np.array([22.07838229,    13.6649195,  -2197.9842622])\n",
    "\n",
    "# We store the new unknown data as feature vector. \n",
    "x_new = np.array([70,47,1])\n",
    "\n",
    "# To find the class of new point, we need to compute the hypothesis value.\n",
    "# Class is dependent on the sign of the obtained value.\n",
    "cat = np.sign(np.dot(log_weight, x_new))\n",
    "\n",
    "# Relate the sign of the value to a category\n",
    "# Class = 1 = Admitted, Class = -1 = Rejected \n",
    "\n",
    "if cat == 1:\n",
    "    print('Student Admitted; Class 1')\n",
    "\n",
    "if cat == -1:\n",
    "    print('Student Rejected; Class -1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.C Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result of part A, we say the our classifier doesn't exactly classifiy al the points correctly. One reason could be that the given dataset is not linearly separable and we may have to resort to nonlinear classification algorithm for this. \n",
    "\n",
    "But it would a good idea to measure (and compare) the Quality of Classifier. After the weights are learned, we can use them to predict the class of our given input points. We them compare the **Predicted Class label** with the **Given Class label**. If they match, then there is no problem. If they differ, we consider that as a false prediction by our  model. To compute the Accuracy of our model take ratio of Correctly classified points and Total Points in training dataset.\n",
    "\n",
    "$$\n",
    "Accuracy = 1 - \\frac{wrong}{Total} \n",
    "$$\n",
    "\n",
    "wrong = incorreclty classified points\n",
    "\n",
    "Total = Total points in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "##  Model's Prediction Accuracy  ##\n",
    "###################################\n",
    "\n",
    "# Suppose the learned weight is calculated from part A of this problem\n",
    "log_weight = np.array([22.07838229,    13.6649195,  -2197.9842622])\n",
    "\n",
    "# Compute the prediction of each Input Point and store in a new array.\n",
    "y_new = np.array([])\n",
    "\n",
    "for each in X:\n",
    "    y_temp = np.sign(np.dot(log_weight, each))\n",
    "    y_new = np.append(y_new, [y_temp])\n",
    "\n",
    "# Compare the predicted value with the Actual Output label class. If doesn't match, consider...\n",
    "#... this as wrong prediction\n",
    "wrong = 0\n",
    "\n",
    "for p in range(m):\n",
    "    if y[p] != y_new[p]:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "# Accuracy = (Correct_predictions / Total Predictions) * 100 %\n",
    "print('Accuracy of Learned model is {} %.'.format(100- (100*wrong/m)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
