{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning :  Problem 5\n",
    "## Non-Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "These exercises are based on the Stanford Machine Learning Course [CS229](http://cs229.stanford.edu) of Andrew Ng. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University.\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: .[MatplotLib](https://matplotlib.org/). [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will expand our knowledge of Linear regression to Non-Linear Regression. The intitial part of the problem remain similar where user has to extract the data from given file and segregate it into 2 variables. User can also plot these recovered points (just as did in Problem 1) and see that a straight line would not be the best possible fit for given dataset. Therefore we resort to Non-Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of NLR\n",
    "\n",
    "Instead of creating new input features $ x_i'$ from old ones $ x_i $ (which we did in LR by adding a column of 1s to every element of input datapoint), it is common to use a feature\n",
    "function which maps from $ x_i$ to $ x_i'$. For eg, the feature vector for NLR of degree 2 looks like below:\n",
    "\n",
    "$$\n",
    "\\phi = \\left(\\begin{array}{cc} \n",
    "x_i Â²\\\\\n",
    "x_i \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The objective of gradient descent remain the same which is to minimize the squared loss defined by equation below:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m}\\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big)^2\n",
    "$$\n",
    "\n",
    "where the hypthesis function $h_w(x)$ is given by the linear model\n",
    "\n",
    "$$\n",
    "h_w(x) = w^T\\phi = w_0 + w_1 x + w_2 x^2 + ....\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A Second Degree NLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## \n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 15.09.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Dependencies imported\n",
    "\n",
    "# For vector computations and notations\n",
    "import numpy as np \n",
    "\n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining Solving Parameters\n",
    "\n",
    "alpha = 1 * 10 ** -1\n",
    "acc = 10 ** -5\n",
    "\n",
    "degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in a .csv file which can be read in a similar way as seen in Problem 1. User can use this set of datapoints to generate a linear regression line but that would not be the best fit. This part of the problem is not shown here but can be easily reproduced. Similarly, plotting in this Notebook is also done in Matplotlib library of Python, documentation can be found here: https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##       Read Data        ##\n",
    "############################\n",
    "\n",
    "# file in the same directory\n",
    "data = np.genfromtxt('nl_data.csv', delimiter=',' )\n",
    "\n",
    "# Initiate a Empty Input feature\n",
    "x_data = np.array([])\n",
    "\n",
    "# Fill values from read-data\n",
    "for each in data:\n",
    "    x_data = np.append(x_data , [each[0]])\n",
    "\n",
    "# Initiate a Empty Output feature\n",
    "y_data = np.array([])\n",
    "\n",
    "for each in data:\n",
    "    y_data = np.append(y_data , [each[1]])\n",
    "\n",
    "# Size of data\n",
    "m = y_data.size\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "##    Visualize Data      ##\n",
    "############################\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_data , y_data ,'ro', ms=10, mec='k')\n",
    "plt.ylabel('Normalized Age (in years)')\n",
    "plt.xlabel('Normalized Height (in cms)')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data\n",
    "\n",
    "In the whole scope of Machine Learning, we deal with variable of different scalings. For example, X variable ranges in 10^4 whereas Y variable ranges from just 1 to 10. In this case, X variable will have a much larger weight in the calculations which will be dominated by X. This my very often results in erroneous prediction if not utterly false values. Transforming the data to comparable scales can prevent this problem. Typical data standardization procedures equalize the range and/or data variability. There are many methods to Normalize the data out of which we choose the following: \n",
    "\n",
    "$$\n",
    "x''^{(i)} = \\frac{x^{(i)} - min(X)}{max(X) - min(X)} \n",
    "$$\n",
    "\n",
    "where $min(X)$ = minimum value in the whole set of Input Variable\n",
    "and   $max(X)$ = minimum value in the whole set of Input Variable.\n",
    "\n",
    "Similar transformation technique is applied to Output Variable(Y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Noramalize data     ##\n",
    "############################\n",
    "\n",
    "# Finding the max and min to normalize the data.\n",
    "max_x = max(x_data)\n",
    "min_x = min(x_data)\n",
    "max_y = max(y_data)\n",
    "min_y = min(y_data)\n",
    "\n",
    "# Each data point in X and Y are normalized as follows\n",
    "# New_x = (Old_x - Min_x) / (Max_x - Min_x) \n",
    "\n",
    "for f in range(m):\n",
    "    x_data[f] = (x_data[f] - min_x) / (max_x - min_x)\n",
    "\n",
    "for h in range(m):\n",
    "    y_data[h] = (y_data[h] - min_y) / (max_y - min_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we will define the following functions:\n",
    "1. Producing a feature function (\\phi) from given input point.\n",
    "2. Produce a hypothesis value (which is dot product of \\phi and weight vector)\n",
    "3. Function to compute the gradient (as denoted in equation above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Feature Function    ##\n",
    "############################\n",
    "\n",
    "def feature(x):\n",
    "    global degree\n",
    "    # produces a feature vector with given inputs \n",
    "\n",
    "    phi_temp = np.array([])\n",
    "    \n",
    "    # depending on degree, make a feauture vector with powers of input x\n",
    "    for p in range(degree + 1):\n",
    "        phi_temp = np.append(phi_temp, [x ** p]) # storing in an array\n",
    "\n",
    "    return phi_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Hypothesis Func     ##\n",
    "############################\n",
    "\n",
    "def hypo(ix,w):\n",
    "    # function for dot product\n",
    "    h = np.dot(ix,w)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Sqaured Loss Func)   ##\n",
    "############################\n",
    "\n",
    "def sq_gradient(x,w,y):\n",
    "    \n",
    "    # Valid for sqaure loss only\n",
    "    # See analytical solution\n",
    "    \n",
    "    sq_grad = feature(x)*(hypo(feature(x), w) - y) \n",
    "    \n",
    "    return sq_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the square loss function we can implement the gradient descent routine. First, write a function $\\nabla J(w)$ which returns the gradient of the sqaure loss function. The gradient is just a vector with all the partial derivatives\n",
    "\n",
    "$$\n",
    "\\nabla J(w) = \\bigg[\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_d} \\bigg]^T\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big) x_j^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the functions are defined then the job is pretty straight forward as seen in Problem 1. The gradient descent function can be written with weight changes after each iteration. Implement gradient descent in the function `gradient_descent(x,y,w,acc,alpha)`.\n",
    "Recall the update rule of gradient descent which is\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla J(w^{(k)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent(x,y,w,acc):\n",
    "    global itr, m\n",
    "\n",
    "    delta_w = np.ones(degree + 1) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(acc < abs(a) for a in delta_w):\n",
    "        \n",
    "        sq_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(m):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            sq_g = sq_g + sq_gradient(x[a],w,y[a])/m\n",
    "            a = a+1\n",
    "       \n",
    "        delta_w = alpha * sq_g \n",
    "        \n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "    \n",
    "    return w, itr\n",
    "\n",
    "# Call function to solve\n",
    "\n",
    "weight = np.zeros(degree+1)\n",
    "\n",
    "sq_weight, sq_itr = gradient_decent(x_data, y_data, weight, acc)\n",
    "print('The optimized weight vector is {}.'.format(sq_weight))\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(acc,alpha))\n",
    "print('Total iterations done = {}'.format(sq_itr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm will run its course and after each iteration the weight vector will be modified to get a lower Gradient of the Sqaured Loss function. Finally the weight vector is converged. To plot the learned vector as a curve we have to compute the Output value from the hypothesis function and the same is plotted along with the Input data points. Note that we sort the Input Variables to get a continuous graph of Predicted Outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "# sorting to get a continuous polynomial output\n",
    "x_data.sort()\n",
    "\n",
    "# Compute Output based on Learned Weight vector\n",
    "new_y = np.array([])\n",
    "for every in x_data:\n",
    "    new_y = np.append(new_y, [hypo(feature(every), sq_weight)]) \n",
    "\n",
    "#Plot in the same graph\n",
    "plt.plot(x_data, new_y, '--')\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.title('Non-Linear Regression with Squared Loss Func')\n",
    "plt.legend(['Training data', 'Non - Linear regression'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B Observe higher order NLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to simultaneously solve for non-linear regression of different degrees. As the degree of the model increase the regression tries to pass as many points from the training data and Sqaured Error is minimized. So extend our solution of Part A, we just keep $degree$ as a input to the function. Accordingly, $Feature Function$ and $Gradient Function$ is changed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Feature Function    ##\n",
    "############################\n",
    "\n",
    "def feature(x,degree):\n",
    "    #global degree\n",
    "    # produces a feature vector with given inputs \n",
    "\n",
    "    phi_temp = np.array([])\n",
    "    \n",
    "    # depending on degree, make a feauture vector with powers of input x\n",
    "    for p in range(degree + 1):\n",
    "        phi_temp = np.append(phi_temp, [x ** p]) # storing in an array\n",
    "\n",
    "    return phi_temp\n",
    "\n",
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Sqaured Loss Func)   ##\n",
    "############################\n",
    "\n",
    "def sq_gradient(x,w,y,degree):\n",
    "    \n",
    "    # Valid for sqaure loss only\n",
    "    # See analytical solution\n",
    "    \n",
    "    sq_grad = feature(x,degree)*(hypo(feature(x,degree), w) - y) \n",
    "    \n",
    "    return sq_grad\n",
    "\n",
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent(x,y,w,acc,degree):\n",
    "    global itr, m\n",
    "\n",
    "    delta_w = np.ones(degree + 1) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(acc < abs(a) for a in delta_w):\n",
    "        \n",
    "        sq_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(m):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            sq_g = sq_g + sq_gradient(x[a], w, y[a], degree)/m\n",
    "            a = a+1\n",
    "       \n",
    "        delta_w = alpha * sq_g \n",
    "        \n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "    \n",
    "    return w, itr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is done in batch, where all degrees for which regression model is required are stored in a list and thereby iteratively solved. Eg: In our case we solve of for 5 different values.\n",
    "\n",
    "$$\n",
    "degree = [ 3, 4, 6, 7, 8 ]\n",
    "$$\n",
    "\n",
    "Correspond to each value of degree, we will get a Weight Vector and Number of Iterations it took for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##     Bunch Solving      ##\n",
    "############################\n",
    "\n",
    "degree = [ 3, 4, 6, 7, 8 ]\n",
    "\n",
    "sq_weight = [0] * len(degree)\n",
    "sq_itr = [0] * len(degree)\n",
    "\n",
    "print('Solving.............')\n",
    "for b in range(len(degree)):\n",
    "\n",
    "    weight = np.zeros(degree[b] + 1)\n",
    "\n",
    "    sq_weight[b], sq_itr[b] = gradient_decent(x_data, y_data, weight, acc, degree[b])\n",
    "\n",
    "\n",
    "print('The weight vectors are optimized for each degree specification.')\n",
    "\n",
    "# Uncomment the next line to print weight vector of a specific degree. Carefull with indexing.\n",
    "#print('The optimized weight vector is {}.'.format(sq_weight[i]))\n",
    "\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(acc,alpha))\n",
    "print('Total iterations done = {}. (In same order as degree specification)'.format(sq_itr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While plotting also, we take the Solutions of Weight Vector one by one and compute the Predicted Output based on the respective model of that degree. Note that we sort the Input Variables to get a continuous graph of Predicted Outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "# sorting to get a continuous polynomial output\n",
    "x_data.sort()\n",
    "\n",
    "# Compute Output based on Learned Weight vector\n",
    "new_y = [np.array([])]*len(degree)\n",
    "\n",
    "for k in range(len(degree)):\n",
    "\n",
    "    # for each degree\n",
    "    for every in x_data:\n",
    "        new_y[k] = np.append(new_y[k], [hypo(feature(every,degree[k]), sq_weight[k])]) \n",
    "\n",
    "for v in range(len(degree)):\n",
    "    # Plot for each degree\n",
    "    plt.plot(x_data, new_y[v])\n",
    "\n",
    "plt.legend(['Training data', 'Degree 3', 'Degree 4', 'Degree 6', 'Degree 7', 'Degree 8'])\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.title('Non-Linear Regression with Varible degrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.C Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part B, we learned that as model degree is increased in tries to fit each datapoint with more accuracy. But this comes at a cost. Generally, till a certain complexity it is benefecial, but after that the model starts to Overfit. This means that the regression model is trained for the given set of data points. It may produce error less outputs for the known points but it doesn't behave very well to unknown and unseen new data. In this part we divide the given dataset into 2 parts: Training Data and Testing Data. Different model of varying complexity are trained with the Training Dataset and their efficiency is evaluated by computing the error they produce when encountered by Test Datapoints. This method is often termed as 'Cross Validation'. We use a python library SCIKIT LEARN (https://scikit-learn.org/stable/index.html) to divide datapoints into 2 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "############################\n",
    "##       Read Data        ##\n",
    "############################\n",
    "\n",
    "# file in the same directory\n",
    "data = np.genfromtxt('nl_data.csv', delimiter=',' )\n",
    "\n",
    "train_data , test_data = model_selection.train_test_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained on just the Training datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a Empty Input feature\n",
    "x_data = np.array([])\n",
    "\n",
    "# Fill values from read-data\n",
    "for each in train_data:\n",
    "    x_data = np.append(x_data , [each[0]])\n",
    "\n",
    "# Initiate a Empty Output feature\n",
    "y_data = np.array([])\n",
    "\n",
    "for each in train_data:\n",
    "    y_data = np.append(y_data , [each[1]])\n",
    "\n",
    "# Size of data\n",
    "m = y_data.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as done in part B, we solve for a bunch of possibilities. The solving function is exactly the same and can be easily reproduced.\n",
    "\n",
    "$$\n",
    "degree = [ 2, 3, 4, 6, 8, 10, 12, 15, 18, 20, 22, 24, 26, 28, 30]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Models\n",
    "\n",
    "To evaluate the model on the Test Datapoints, we make Input and Outputs arrays. Normalizing on these points is done similar as done above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##        Test data       ##\n",
    "############################\n",
    "\n",
    "# Initiate a Empty Input feature\n",
    "tx_data = np.array([])\n",
    "\n",
    "# Fill values from read-data\n",
    "for each in test_data:\n",
    "    tx_data = np.append(tx_data , [each[0]])\n",
    "\n",
    "# Initiate a Empty Output feature\n",
    "ty_data = np.array([])\n",
    "\n",
    "for each in test_data:\n",
    "    ty_data = np.append(ty_data , [each[1]])\n",
    "\n",
    "# Normalize Test data with concept as applied in Training Data\n",
    "max_tx = max(tx_data)\n",
    "min_tx = min(tx_data)\n",
    "max_ty = max(ty_data)\n",
    "min_ty = min(ty_data)\n",
    "\n",
    "for f in range(len(ty_data)):\n",
    "    tx_data[f] = (tx_data[f] - min_tx) / (max_tx - min_tx)\n",
    "\n",
    "for h in range(len(ty_data)):\n",
    "    ty_data[h] = (ty_data[h] - min_ty) / (max_ty - min_ty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next write a function to compute the error between Hypothesis Value and Actual Output Value\n",
    "\n",
    "$$\n",
    "Mean Error = \\frac{1}{p}\\sum_{i=1}^p \\big( h_w(x^{(i)}) - y^{(i)}\\big)^2\n",
    "$$\n",
    "\n",
    "where $p$ is the count of Test Datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##      Error function    ##\n",
    "############################\n",
    "\n",
    "def errors(y,yp):\n",
    "    # Computes the mean error between Actual Output Value vs the Predicted Value\n",
    "    total_error = 0\n",
    "\n",
    "    for r in range(len(y)):\n",
    "        total_error = total_error + ( ( y[r] - yp[r] ) ** 2 )\n",
    "\n",
    "    mean_err = total_error / (len(y))\n",
    "\n",
    "    return mean_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Error Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Plot Error Curve    ##\n",
    "############################\n",
    "\n",
    "# Compute Output based on Learned Weight vector\n",
    "new_y = [np.array([])]*len(degree)\n",
    "\n",
    "for k in range(len(degree)):\n",
    "    # for each degree\n",
    "    for every in tx_data:\n",
    "        new_y[k] = np.append(new_y[k], [hypo(feature(every,degree[k]), sq_weight[k])]) \n",
    "\n",
    "mean_err = [np.array([])]*len(degree)\n",
    "\n",
    "for u in range(len(degree)):\n",
    "    mean_err[u] = errors(ty_data, new_y[u])\n",
    "\n",
    "plt.plot(degree,mean_err)\n",
    "\n",
    "plt.xlabel('Degree of learned non-linear configuration')\n",
    "plt.ylabel('Mean error on Unseen Data')\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.legend(['Error Curve'])\n",
    "plt.xticks(np.linspace(1,30,30))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
