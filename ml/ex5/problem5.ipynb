{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will expand our knowledge of Linear regression to Non-Linear Regression. The intitial part of the problem remain similar where user has to extract the data from given file and segregate it into 2 variables. User can also plot these recovered points (just as did in Problem 1) and see that a straight line would not be the best possible fit for given dataset. Therefore we resort to Non-Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of NLR\n",
    "\n",
    "Instead of creating new input features $ x_i'$ from old ones $ x_i $ (which we did in LR by adding a column of 1s to every element of input datapoint), it is common to use a feature\n",
    "function which maps from $ x_i$ to $ x_i'$. For eg, the feature vector for NLR of degree 2 looks like below:\n",
    "\n",
    "$$\n",
    "\\phi = \\left(\\begin{array}{cc} \n",
    "x_i Â²\\\\\n",
    "x_i \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "The objective of gradient descent remain the same which is to minimize the squared loss\n",
    "$$\n",
    "J(w) = \\frac{1}{m}\\sum_{i=1}^n \\big( h_w(x^{(i)}) - y^{(i)}\\big)^2\n",
    "$$\n",
    "where the hypthesis function $h_w(x)$ is given by the linear model\n",
    "$$\n",
    "h_w(x) = w^T\\phi = w_0 + w_1 x + w_2 x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##       Created by: SAHIL ARORA        ## \n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 15.09.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Dependencies imported\n",
    "\n",
    "# For vector computations and notations\n",
    "import numpy as np \n",
    "\n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining Solving Parameters\n",
    "\n",
    "# Range 0.0001 to 0.00001\n",
    "alpha = 1 * 10 ** -4\n",
    "acc = 10 ** -5\n",
    "\n",
    "degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can refer to Problem 1 for code on how to read data from given .txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the square loss function we can implement the gradient descent routine. A function $\\nabla J(w)$ which returns the gradient of the sqaure loss function. The gradient is just a vector with all the partial derivatives\n",
    "$$\n",
    "\\nabla J(w) = \\bigg[\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_d} \\bigg]^T\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big) x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we will define the following functions:\n",
    "1. Producing a feature function (\\phi) from given input point.\n",
    "2. Produce a hypothesis value (which is dot product of \\phi and weight vector)\n",
    "3. Function to compute the gradient (as denoted in equation above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "##    Feature Function    ##\n",
    "############################\n",
    "\n",
    "def feature(x):\n",
    "    global degree\n",
    "    # produces a feature vector with given inputs \n",
    "\n",
    "    phi_temp = np.array([])\n",
    "    \n",
    "    # depending on degree, make a feauture vector with powers of input x\n",
    "    for p in range(degree + 1):\n",
    "        phi_temp = np.append(phi_temp, [x ** p]) # storing in an array\n",
    "\n",
    "    return phi_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Hypothesis Func     ##\n",
    "############################\n",
    "\n",
    "def hypo(ix,w):\n",
    "    # function for dot product\n",
    "    h = np.dot(ix,w)\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Sqaured Loss Func)   ##\n",
    "############################\n",
    "\n",
    "def sq_gradient(x,w,y):\n",
    "    \n",
    "    # Valid for sqaure loss only\n",
    "    # See analytical solution\n",
    "    \n",
    "    sq_grad = feature(x)*(hypo(feature(x), w) - y) \n",
    "    \n",
    "    return sq_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the functions are defined then the job is pretty straight forward as seen in Problem 1. The gradient descent function can be written with weight changes after each iteration. Implement gradient descent in the function `gradient_descent(x,y,w,acc,alpha)`.\n",
    "Recall the update rule of gradient descent which is\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla J(w^{(k)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent(x,y,w,acc):\n",
    "    global itr, m\n",
    "\n",
    "    delta_w = np.ones(degree + 1) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(acc < abs(a) for a in delta_w):\n",
    "        \n",
    "        sq_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(m):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            sq_g = sq_g + sq_gradient(x[a],w,y[a])/m\n",
    "            a = a+1\n",
    "       \n",
    "        delta_w = alpha * sq_g \n",
    "        \n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "    \n",
    "    return w, itr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the convergency has reached, we print out the result and plot the graph so visualize the results. Note that we sort the input datapoints to get a continous polynomial graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.zeros(degree+1)\n",
    "\n",
    "sq_weight, sq_itr = gradient_decent(x_data, y_data, weight, acc)\n",
    "print('The optimized weight vector is {}.'.format(sq_weight))\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(acc,alpha))\n",
    "print('Total iterations done = {}'.format(sq_itr))\n",
    "\n",
    "\n",
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "# sorint to get a continuous polynomial output\n",
    "x_data.sort()\n",
    "\n",
    "new_y = np.array([])\n",
    "for every in x_data:\n",
    "    new_y = np.append(new_y, [hypo(feature(every), sq_weight)]) \n",
    "\n",
    "plt.plot(x_data, new_y, '-')\n",
    "plt.legend(['Training data', 'Non - Linear regression'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
