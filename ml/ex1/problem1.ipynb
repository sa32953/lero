{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this Jupyter Notebook is to guide you with steps folowed to reach a appropriate solution. Simply copying the data from this notebook for solving problem sheets is highly discouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first part of the problem, user needs to read the data given the '.txt' format. The given data is read by 'np.loadtext' command with appropritate delimiter specifications as seen in the given text file. Once the data is stored in a variable, user has to define 2 empty arrays (1 for Input and 1 for Output). Using a simple 'for' loop, data is appended in respective arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################\n",
    "##       Created by: SAHIL ARORA        ## \n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 05.09.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Dependencies imported\n",
    "\n",
    "# For vector computations and notations\n",
    "import numpy as np \n",
    "\n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining Solving Parameters\n",
    "\n",
    "alpha = 0.01\n",
    "error = 10 ** -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving parameters are defined in this section. We take learing rate as 0.01 (as specified in the problem). For convergence we take the error between current weight vector and weight verctor after next iteration. If this error is more than 10^-4, then the process continues. As soon as it falls below the limit, we consider that convergency has reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##       Read Data        ##\n",
    "############################\n",
    "\n",
    "# file in the same directory\n",
    "data = np.loadtxt('data.txt', delimiter=',' )\n",
    "\n",
    "# Initiate a Empty Input feature\n",
    "x = np.array([])\n",
    "\n",
    "# Fill values from read-data\n",
    "for each in data:\n",
    "    x = np.append(x , [each[0]])\n",
    "\n",
    "# Initiate a Empty Output feature\n",
    "y = np.array([])\n",
    "\n",
    "for each in data:\n",
    "    y = np.append(y , [each[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting is done using a python library 'matplotlib'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Visualize Data      ##\n",
    "############################\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x,y,'ro', ms=10, mec='k')\n",
    "plt.ylabel('Profit in $10,000')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing Gradient Descent algorith, please refer to the slides to get a hold of how the feautre function looks for Linear Regression problems. User needs to arrange the data stored in 'X' input varible to convert it into a feature function. Simultaneously, the weight vector 'W' is initialized as [0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##      Arrange Data      ##\n",
    "############################\n",
    "\n",
    "# Size of data\n",
    "m = y.size\n",
    "\n",
    "# Make a linear feature vector\n",
    "x = np.stack([x, np.ones(m)], axis=1)\n",
    "\n",
    "# Initiate weight vector\n",
    "w = np.array([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User should by now be familiar with analytical solution of a Gradient Descent Problem and should be able to write the gradient for a Sqaure Loss function. Kindly refer to lecture notes in case of ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Sqaured Loss Func)   ##\n",
    "############################\n",
    "\n",
    "def sq_gradient(x,w,y):\n",
    "    \n",
    "    # Valid for sqaure loss only\n",
    "    # See analytical solution\n",
    "    sq_grad = x*(np.dot(x,w) - y) \n",
    "    \n",
    "    return sq_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the pseudo code as given in lecture, the main function is written as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent():\n",
    "    global itr, w, m, error\n",
    "\n",
    "    delta_w = np.array([1,1]) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(error < abs(a) for a in delta_w):\n",
    "        \n",
    "        sq_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(len(y)):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            sq_g = sq_g + sq_gradient(x[a],w,y[a])/m\n",
    "            a = a+1\n",
    "        \n",
    "        delta_w = alpha * sq_g\n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "    \n",
    "    return w, itr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the weight vector is computed and is plotted along with the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_weight, sq_itr = gradient_decent()\n",
    "print('The optimized weight vector is {}.'.format(sq_weight))\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(error,alpha))\n",
    "print('Total iterations done = {}'.format(sq_itr))\n",
    "\n",
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "plt.plot(x[:, 0], np.dot(x, sq_weight), '-')\n",
    "plt.legend(['Training data', 'Linear regression'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part we use subgradient to proceed with the algorithm. Out of many possible sub-gradient we choose the one which gives (grad = x*sign(x.w -y)). Accordingly, the function for computing gradient is changes as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Absolute Loss Func)  ##\n",
    "############################\n",
    "\n",
    "def ab_gradient(x,w,y):\n",
    "    \n",
    "    # Valid for Absolute loss only\n",
    "    # See analytical solution\n",
    "    ab_grad = x*np.sign((np.dot(x,w) - y))\n",
    "    \n",
    "    return ab_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in the main function can be reproduced easily. It is not shown here and is left for the user to complete. To plot both the regression lines in one plot we use the results of part B. The following set of lines is to be un-commented and the result of weight vector is to be added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From earlier part B. Uncomment the next line and substitute the values from part B.\n",
    "\n",
    "#sq_weight = np.array([1.13774908,-3.34547133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "plt.plot(x[:, 0], np.dot(x, ab_weight), '-')\n",
    "plt.plot(x[:, 0], np.dot(x, sq_weight), '.')\n",
    "plt.legend(['Training data', 'Ab_Linear regression', 'Sq_Linear regression'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
