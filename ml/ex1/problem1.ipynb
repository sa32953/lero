{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning :  Problem 1\n",
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "These exercises are based on the Stanford Machine Learning Course [CS229](http://cs229.stanford.edu) of Andrew Ng. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University.\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: .[MatplotLib](https://matplotlib.org/). [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A Visualize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this Jupyter Notebook is to guide you with steps folowed to reach a appropriate solution. Simply copying the data from this notebook for solving problem sheets is highly discouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first part of the problem, user needs to read the data given the '.txt' format. The given data is read by 'np.loadtext' command with appropritate delimiter specifications as seen in the given text file. Once the data is stored in a variable, user has to define 2 empty arrays (1 for Input and 1 for Output). Using a simple 'for' loop, data is appended in respective arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##       Created by: SAHIL ARORA        ## \n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 05.09.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Dependencies imported\n",
    "\n",
    "# For vector computations and notations\n",
    "import numpy as np \n",
    "\n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining Solving Parameters\n",
    "\n",
    "alpha = 0.01\n",
    "acc = 10 ** -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving parameters are defined in previous section. We take learing rate as 0.01 (as specified in the problem). For convergence we take the error between current weight vector and weight vector after next iteration. If this error is more than 10^-4, then the iterations continue. As soon as it falls below the limit, we consider that convergency has reached because w vector is not changing much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##       Read Data        ##\n",
    "############################\n",
    "\n",
    "# file in the same directory\n",
    "data = np.loadtxt('data.txt', delimiter=',' )\n",
    "\n",
    "# Initiate a Empty Input feature\n",
    "x = np.array([])\n",
    "\n",
    "# Fill values from read-data\n",
    "for each in data:\n",
    "    x = np.append(x , [each[0]])\n",
    "\n",
    "# Initiate a Empty Output feature\n",
    "y = np.array([])\n",
    "\n",
    "for each in data:\n",
    "    y = np.append(y , [each[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting is done using a python library 'matplotlib'. See documention of the library at https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Visualize Data      ##\n",
    "############################\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x,y,'ro', ms=10, mec='k')\n",
    "plt.ylabel('Profit in $10,000')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "#plt.show()\n",
    "\n",
    "# Size of data\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing Gradient Descent algorith, please refer to the slides to get a hold of how the feature function looks for Linear Regression problems. The feature vector for LR looks like below:\n",
    "\n",
    "$$\n",
    "\\phi = \\left(\\begin{array}{cc} \n",
    "x_i \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "User needs to arrange the data stored in 'X' input varible to convert it into a feature function. Simultaneously, the weight vector 'W' is initialized as [0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##      Arrange Data      ##\n",
    "############################\n",
    "\n",
    "# Make a linear feature vector\n",
    "x = np.stack([x, np.ones(m)], axis=1)\n",
    "\n",
    "# Initiate weight vector\n",
    "w = np.array([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update equations\n",
    "\n",
    "The objective of gradient descent is to minimize the mean squared loss\n",
    "$$\n",
    "J(w) = \\frac{1}{m}\\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big)^2\n",
    "$$\n",
    "where the hypthesis function $h_w(x)$ is given by the linear model\n",
    "$$\n",
    "h_w(x) = w^Tx = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "The parameters of the model are the $w_j$ values. \n",
    "These are the values that we will adjust to minimize the gradient of Loss function. This will be done using the gradient descent algorithm which performs the update\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= w_j - \\frac{\\alpha}{m} \\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big) x_j^{(i)}\\\\\n",
    "&= w_j - \\frac{\\alpha}{m} \\frac{\\partial J(w)}{\\partial w_j} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with each step of gradient descent, your parameters $w_j$ come closer to the optimal values that will achieve the lowest gradient $\\nabla J(w)$. Here, $\\alpha$ is called the learning rate.\n",
    "You may notice that we divided squared error by the number of samples $m$. Alternatively we may have to adjust the learning rate for different sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the square loss function we can implement the gradient descent routine. First, write a function $\\nabla J(w)$ which returns the gradient of the sqaure loss function. The gradient is just a vector with all the partial derivatives\n",
    "\n",
    "$$\n",
    "\\nabla J(w) = \\bigg[\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_d} \\bigg]^T\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Below is a function with the inner part of the summation sysmbol. The addition for m samples can be done in the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Sqaured Loss Func)   ##\n",
    "############################\n",
    "\n",
    "def sq_gradient(x,w,y):\n",
    "\n",
    "    # Valid for sqaure loss only\n",
    "    # See analytical solution\n",
    "    \n",
    "    sq_grad = x*(np.dot(x,w) - y) \n",
    "    \n",
    "    return sq_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main function, user can deploy the Pseudo code as seen in lecture.\n",
    "\n",
    "You will implement gradient descent in the function `gradient_descent(x,y,w,acc,alpha)`.\n",
    "Recall the update rule of gradient descent which is\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla J(w^{(k)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent(x,y,w,acc):\n",
    "    global itr, m, error\n",
    "\n",
    "    delta_w = np.array([1,1]) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(acc < abs(a) for a in delta_w):\n",
    "        \n",
    "        sq_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(m):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            sq_g = sq_g + sq_gradient(x[a],w,y[a])/m\n",
    "            a = a+1\n",
    "        \n",
    "        \n",
    "        delta_w = alpha * sq_g \n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "    \n",
    "    return w, itr\n",
    "\n",
    "# Call function to solve\n",
    "\n",
    "sq_weight, sq_itr = gradient_decent(x,y,w,acc)\n",
    "print('The optimized weight vector is {}.'.format(sq_weight))\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(acc,alpha))\n",
    "print('Total iterations done = {}'.format(sq_itr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm will run its course and after each iteration the weight vector will be modified to get a lower Gradient of the Sqaured Loss function. Finally the weight vector is converged. To plot the learned vector as a curve we have to compute the Output value from the hypothesis function and the same is plotted along with the Input data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "plt.plot(x[:, 0], np.dot(x, sq_weight), '-')\n",
    "plt.legend(['Training data', 'Linear regression'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.C Use Absolute Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we add the given outliers points to the Input Array and the Output Array. The objective of this part is to compare results of Square Loss Func and Absolute Loss Func. Using a 'for' loop the outlier points are appended: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add some Outliers points\n",
    "outlier = [[17.5,0], [18.5,0], [19.5,0]]\n",
    "\n",
    "for d in range(len(outlier)):\n",
    "\n",
    "    x = np.append(x , [outlier[d][0]])\n",
    "    y = np.append(y , [outlier[d][1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with Absolute Loss function is that it has no defined Gradient. Nevertheless, for this part we can use subgradient to proceed with the algorithm. Out of many possible sub-gradient we choose the one which gives Gradient $\\nabla J(w^{(k)}$ as\n",
    "\n",
    "$$\n",
    "\\nabla J(w^{(k)}) = x_j^{(i)} * sign(h_w(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "\n",
    "Accordingly, the function for computing gradient is changed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Absolute Loss Func)  ##\n",
    "############################\n",
    "\n",
    "def ab_gradient(x,w,y):\n",
    "    \n",
    "    # Valid for Absolute loss only\n",
    "    # See analytical solution\n",
    "    ab_grad = x*np.sign((np.dot(x,w) - y))\n",
    "    \n",
    "    return ab_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in the main function can be reproduced easily. It is not shown here and is left for the user to complete. To plot both the regression lines in one plot we use the results of part B which are precomputed with ouliers. This computation is not shown in this Notebook but can be easily reproduced by just adding Outliers and Solving Part B again. The following set of lines is to be un-commented and the result of weight vector is to be added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From earlier part B. Uncomment the next line and substitute the values.\n",
    "\n",
    "\n",
    "#If solved with Squared Loss with Outliers, then weight is: \n",
    "#sq_weight = np.array([0.7260848  , -0.17531625])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "x[:, 0].sort()\n",
    "plt.plot(x[:, 0], np.dot(x, ab_weight), '-')\n",
    "plt.plot(x[:, 0], np.dot(x, sq_weight), '--')\n",
    "plt.legend(['Training data', 'Ab_Linear regression', 'Sq_Linear regression'])\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.title('Linear Regression with Absolute Loss Func')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.D Observe Convergence\n",
    "\n",
    "The interval of variation is given in the problem. We make a new variable that has all possible of Learning Rate as a list. Subsequently, for each one of these we solve with Gradient Descent algorithm and store the results of 'Iterations it took for convergence' in a list. Further this list can be plotted using matplotlib library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Solving...........')\n",
    "alphas = [0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02]\n",
    "\n",
    "sq_weight = [0] * len(alphas)\n",
    "sq_itr = [0] * len(alphas)\n",
    "\n",
    "for b in range(len(alphas)):\n",
    "    sq_weight[b], sq_itr[b] = gradient_decent(x,y,w,acc,alphas[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Iteration Var.   ##\n",
    "############################\n",
    "\n",
    "print('Solved. See plot for variation.')\n",
    "plt.plot(alphas, sq_itr, '-')\n",
    "plt.ylabel('Number of Iterations for convergance')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.legend(['Convergence Iterations Vs Learning Rate'])\n",
    "plt.xticks(np.linspace(0.005, 0.02, 7))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
