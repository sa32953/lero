{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of this Jupyter Notebook is to guide you with steps folowed to reach a appropriate solution. Simply copying the data from this notebook for solving problem sheets is highly discouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first part of the problem, user needs to read the data given the '.txt' format. The given data is read by 'np.loadtext' command with appropritate delimiter specifications as seen in the given text file. Once the data is stored in a variable, user has to define 2 empty arrays (1 for Input and 1 for Output). Using a simple 'for' loop, data is appended in respective arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##       Created by: SAHIL ARORA        ## \n",
    "##  HOCHSCHULE- RAVENSBURG WEINGARTEN   ##\n",
    "##          Date: 05.09.2020            ##\n",
    "##########################################\n",
    "\n",
    "# Dependencies imported\n",
    "\n",
    "# For vector computations and notations\n",
    "import numpy as np \n",
    "\n",
    "# For Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining Solving Parameters\n",
    "\n",
    "alpha = 0.01\n",
    "acc = 10 ** -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving parameters are defined in this section. We take learing rate as 0.01 (as specified in the problem). For convergence we take the error between current weight vector and weight verctor after next iteration. If this error is more than 10^-4, then the process continues. As soon as it falls below the limit, we consider that convergency has reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##       Read Data        ##\n",
    "############################\n",
    "\n",
    "# file in the same directory\n",
    "data = np.loadtxt('data.txt', delimiter=',' )\n",
    "\n",
    "# Initiate a Empty Input feature\n",
    "x = np.array([])\n",
    "\n",
    "# Fill values from read-data\n",
    "for each in data:\n",
    "    x = np.append(x , [each[0]])\n",
    "\n",
    "# Initiate a Empty Output feature\n",
    "y = np.array([])\n",
    "\n",
    "for each in data:\n",
    "    y = np.append(y , [each[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting is done using a python library 'matplotlib'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Visualize Data      ##\n",
    "############################\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x,y,'ro', ms=10, mec='k')\n",
    "plt.ylabel('Profit in $10,000')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "#plt.show()\n",
    "\n",
    "# Size of data\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing Gradient Descent algorith, please refer to the slides to get a hold of how the feautre function looks for Linear Regression problems. User needs to arrange the data stored in 'X' input varible to convert it into a feature function. Simultaneously, the weight vector 'W' is initialized as [0,0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update equations\n",
    "\n",
    "The objective of gradient descent is to minimize the gradient\n",
    "$$\n",
    "J(w) = \\frac{1}{m}\\sum_{i=1}^n \\big( h_w(x^{(i)}) - y^{(i)}\\big)^2\n",
    "$$\n",
    "where the hypthesis function $h_w(x)$ is given by the linear model\n",
    "$$\n",
    "h_w(x) = w^Tx = w_0 + w_1 x\n",
    "$$\n",
    "The parameters of the model are the $w_j$ values. \n",
    "These are the values that we will adjust to minimize the gradient function. This will be done using the batch gradient descent algorithm which performs the update\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= w_j - \\frac{\\alpha}{m} \\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big) x_j^{(i)}\\\\\n",
    "&= w_j - \\frac{\\alpha}{m} \\frac{\\partial J(w)}{\\partial w_j} \n",
    "\\end{align}\n",
    "$$\n",
    "with each step of gradient descent, your parameters $w_j$ come closer to the optimal values that will achieve the lowest cost $J(w)$. Here, $\\alpha$ is called the learning rate.\n",
    "You may notice that we divided squared error by the number of samples $n$. Alternatively we have to adjust the learning rate for different sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##      Arrange Data      ##\n",
    "############################\n",
    "\n",
    "# Make a linear feature vector\n",
    "x = np.stack([x, np.ones(m)], axis=1)\n",
    "\n",
    "# Initiate weight vector\n",
    "w = np.array([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User should by now be familiar with analytical solution of a Gradient Descent Problem and should be able to write the gradient for a Sqaure Loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Sqaured Loss Func)   ##\n",
    "############################\n",
    "\n",
    "def sq_gradient(x,w,y):\n",
    "\n",
    "    # Valid for sqaure loss only\n",
    "    # See analytical solution\n",
    "    \n",
    "    sq_grad = x*(np.dot(x,w) - y) \n",
    "    \n",
    "    return sq_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "\n",
    "Now that we have the hypothesis and the gradient function we can implement the gradient descent routine. First, write a function $\\nabla J(w)$ which returns the gradient of the cost function. The gradient is just a vector with all the partial derivatives\n",
    "$$\n",
    "\\nabla J(w) = \\bigg[\\frac{\\partial J(w)}{\\partial w_1} , \\dotsc, \\frac{\\partial J(w)}{\\partial w_d} \\bigg]^T\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\big( h_w(x^{(i)}) - y^{(i)}\\big) x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the pseudo code as given in lecture, the main function is written as follows. \n",
    "\n",
    "You will implement gradient descent in the function `gradient_descent(x,y,w,acc,alpha)`.\n",
    "Recall the update rule of gradient descent which is\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla J(w^{(k)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##    Gradient Descent    ##\n",
    "############################\n",
    "\n",
    "def gradient_decent(x,y,w,acc):\n",
    "    global itr, m, error\n",
    "\n",
    "    delta_w = np.array([1,1]) # initialized randomly\n",
    "\n",
    "    itr = 0\n",
    "    while all(acc < abs(a) for a in delta_w):\n",
    "        \n",
    "        sq_g = 0\n",
    "\n",
    "        # Compute Cumulative gradient\n",
    "        for a in range(m):\n",
    "\n",
    "            # Gradient computation is Normalized by number of data points available\n",
    "            sq_g = sq_g + sq_gradient(x[a],w,y[a])/m\n",
    "            a = a+1\n",
    "        \n",
    "        \n",
    "        delta_w = alpha * sq_g \n",
    "        # alpha is learning rate\n",
    "        w = w - (delta_w)\n",
    "        \n",
    "        itr = itr+1\n",
    "    \n",
    "    return w, itr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the weight vector is computed and is plotted along with the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_weight, sq_itr = gradient_decent(x,y,w,acc)\n",
    "print('The optimized weight vector is {}.'.format(sq_weight))\n",
    "print('Solving criteria with Sq Loss Func: Convergency = {} and Learining Rate = {}'.format(acc,alpha))\n",
    "print('Total iterations done = {}'.format(sq_itr))\n",
    "\n",
    "\n",
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "plt.plot(x[:, 0], np.dot(x, sq_weight), '-')\n",
    "plt.legend(['Training data', 'Linear regression'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part we use subgradient to proceed with the algorithm. Out of many possible sub-gradient we choose the one which gives (grad = x*sign(x.w -y)). Accordingly, the function for computing gradient is changes as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##   Gradient Function    ##\n",
    "##  (Absolute Loss Func)  ##\n",
    "############################\n",
    "\n",
    "def ab_gradient(x,w,y):\n",
    "    \n",
    "    # Valid for Absolute loss only\n",
    "    # See analytical solution\n",
    "    ab_grad = x*np.sign((np.dot(x,w) - y))\n",
    "    \n",
    "    return ab_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in the main function can be reproduced easily. It is not shown here and is left for the user to complete. To plot both the regression lines in one plot we use the results of part B. The following set of lines is to be un-commented and the result of weight vector is to be added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From earlier part B. Uncomment the next line and substitute the values from part B.\n",
    "\n",
    "#sq_weight = np.array([1.13774908,-3.34547133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Regression Line  ##\n",
    "############################\n",
    "\n",
    "plt.plot(x[:, 0], np.dot(x, ab_weight), '-')\n",
    "plt.plot(x[:, 0], np.dot(x, sq_weight), '.')\n",
    "plt.legend(['Training data', 'Ab_Linear regression', 'Sq_Linear regression'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "The interval of variation is given in the problem. We make a new variable that has all possible of Learning Rate as a list. Subsequently, for each one of these we solve with Gradient Descent algorithm and store the results of 'Iterations it took for convergence' in a list. Further this list can be plotted using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02]\n",
    "\n",
    "sq_weight = [0] * len(alphas)\n",
    "sq_itr = [0] * len(alphas)\n",
    "\n",
    "for b in range(len(alphas)):\n",
    "    sq_weight[b], sq_itr[b] = gradient_decent(x,y,w,acc,alphas[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  Plot Iteration Var.   ##\n",
    "############################\n",
    "\n",
    "print('Solved. See plot for variation.')\n",
    "plt.plot(alphas, sq_itr, '-')\n",
    "plt.ylabel('Number of Iterations for convergance')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.grid(b=None, which='major', axis='both')\n",
    "plt.legend(['Convergence Vs Rate'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
