{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods :  Problem 1\n",
    "## Bagging with SCIKIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "These exercises are based on the work by T. Hastie et al. pertaining to the article 'Elements of Statistical Learning', Springer 2009. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University. [Hastie et al.](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py)\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: .[MatplotLib](https://matplotlib.org/). [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Referring to the lecture slides, we learnt that Ensemble Methods are usefull in cases the individual model cannot perform well. Ensemble methods make a combination of models such that the ensemble has a better performance than the individual ones. An ensemble is created by averaging (in case of regression) or voting (in case of\n",
    "classification). Refer the below image for recalling how the Bagging Method work.\n",
    "\n",
    "<img src='./graphic/2.png' width='550' height='550'>\n",
    "\n",
    "In this excercise, we will work with implementation on such ensemble method called **Bagging**. We will be generating a custom dataset which has a high variance. We will then train a regression algorithm with Decision Tree Regressor and compare it with the Bagging regressor using the function from Sklearn library [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html).\n",
    "\n",
    ">\"A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\"\n",
    "\n",
    "<img src='./graphic/1.png' width='550' height='550'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "n_repeat = 5        # Number of iterations for computing expectations\n",
    "n_train = 500       # Size of the training set\n",
    "n_test = 100        # Size of the test set\n",
    "noise = 0.075       # Standard deviation of the noise\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are generating is a 2-D set with points **X** and **y**. We choose the points for **X** as a scaled factor of random number from a uniform distribution ranging from $[0,1)$.\n",
    "\n",
    "$$\n",
    "X = 10*(rand\\in[0,1)) -5\n",
    "$$\n",
    "\n",
    "To generate the **y** points from correspondind **X** points, we define a function **f(x)**.\n",
    "\n",
    "$$\n",
    "f(x)= e^{(-x^2)} + 1.5 e^{-(x-2)^2}\n",
    "$$\n",
    "\n",
    "In this case then, the output points for **y** will be value of **f(x)** plus some noise. The Noise is added by selecting a random number from a Normal Probabilistic Distribution with **zero mean** and $\\sigma$ as standard deviation.\n",
    "\n",
    "$$\n",
    "y = f(x) + N(0,\\sigma)\n",
    "$$\n",
    "where N is a Normal Probabilistic Distribution.\n",
    "\n",
    "Since we wish to evaluate the model, we will split the Training and the Testing set during model fitting and validation. The ratio choosen for the split is 5:1 for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for y points\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return '''Complete code here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of generating 2D set and storing as array\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    \n",
    "    '''Code here'''\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate(n_samples=(n_train+n_test), noise=noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce a random split and Plot points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split('''Code here''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train, y_train, \".g\", label=\"Training Data\")\n",
    "plt.plot(X_test, y_test, \".r\", label=\"Test Data\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B Set up two Estimators\n",
    "\n",
    "Bagging, short for bootstrap aggregating, was introduced by Breiman (1996) to reduce the prediction error of learning algorithms with high variance. To reduce the prediction error, we draw bootstrap copies from the original training dataset and train a regression or classification algorithm (base learner) on each copy.\n",
    "\n",
    "There fist we need a base learner. We are dealing with regression problem, so there exist a number of Regressors, like SGD that we learnt in this course but also Decision Tree Regressor and K-nearest neighbour regressors. The latter two are high-variance regressors that means with small change in the input quantity, a large change in the output is seen. Bagging Methods are better suitable for high variance regressors.\n",
    "\n",
    "So, for this problem, we use the base learner as Decision Tree Regressor from Scikit. \n",
    "\n",
    "### Decision Tree Regressor\n",
    "\n",
    "We will be using a **Decision Tree Regressor** to fit the *training set* of the datapoints. To show the comparison, we do a similar regression but this time with Bagging for *n=10* (default) iterations. Once, both the models are trained, we predict the output values for each point in *test dataset* and compare the values of the true and predicted outputs in terms of Mean Squared Error.\n",
    "\n",
    "> \"Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\"\n",
    "\n",
    "source [Decision Tree Definition at Sklearn](https://scikit-learn.org/stable/modules/tree.html#tree)\n",
    "\n",
    "### Additional reading\n",
    "\n",
    "[Bagging on Low Variance Models](https://towardsdatascience.com/bagging-on-low-variance-models-38d3c70259db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this for exploring the bias-variance decomposition of other\n",
    "# estimators. This should work well for estimators with high variance (e.g.,\n",
    "# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n",
    "# linear models).\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "estimators = [(\"Decision Tree\", dt_reg ),\n",
    "              (\"Bagging(Decision Tree)\", '''Code here''')]\n",
    "\n",
    "n_estimators = len(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train each of the model **n_repeat** number of times. For each iteration, a different set of training and testing set is chosen from the whole dataset. The process goes as follows:\n",
    "\n",
    "1. Choose one of the estimator that need to trained.\n",
    "\n",
    "2. Set up empty arrays for y_prediction and y_error.\n",
    "\n",
    "3. For each iteration ranging in **n_repeat**, split the dataset using **test_train_split** function and train the model on training dataset.\n",
    "\n",
    "4. After the model is trained, use the test split to predict the value of output.\n",
    "\n",
    "5. Compute the MSE error between the predicted output and the actual output of the test split.\n",
    "\n",
    "6. Plot the results and print the mean error for all estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    \n",
    "    # Predictions array\n",
    "    \n",
    "    \n",
    "    # Error Array\n",
    "    '''Code here'''\n",
    "    \n",
    "    plt.subplot(2, n_estimators, n + 1)\n",
    "    \n",
    "    for i in range(n_repeat):\n",
    "        # Fit on randomly selected training data\n",
    "        \n",
    "        '''Code here'''\n",
    "        \n",
    "        # Plot training and predicted data\n",
    "        if i == 0:\n",
    "            plt.plot(X_test, y_test, \".b\", label=\"$y = f(x)+noise$\")\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", label=r\"$\\^y(x)$\")\n",
    "            \n",
    "        else:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.1)\n",
    "   \n",
    "    print(\"{0}  shows a Mean Error of : {1:.4f}\".format(name, np.mean(y_error)))\n",
    "    \n",
    "    # Plotting tools\n",
    "    plt.title(name)\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.grid()\n",
    "    if n == n_estimators - 1:\n",
    "        plt.legend(loc=(1.1, .5))\n",
    "\n",
    "plt.subplots_adjust(right=.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As observed, the mean prediction error in the Bagging Estimator is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can you do to further contribute in this notebook ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
