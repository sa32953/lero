{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods :  Problem 2\n",
    "## Boosting with SCIKIT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "These exercises are based on the work by H. Drucker pertaining to the article “Improving Regressors using Boosting Techniques\" at Monmouth University. The environment of the exercise have been tuned to the theory content taught at Ravensburg Weingarten University. [H. Drucker](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py)\n",
    "\n",
    "We are using the Python programming language. If you don't know Python or if you would like to refresh your memory, take a look at the [Python tutorial](http://docs.python.org/tut/).\n",
    "We will mostly work with NumPy, the fundamental package for scientific computing in Python. Please read the [NumPy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html). In addition, the documention of MatPlotLib and Scipy lib can be found here: .[MatplotLib](https://matplotlib.org/). [Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Referring to the lecture slides, we learnt that Ensemble Methods are usefull in cases the individual model cannot perform well. Ensemble methods make a combination of models such that the ensemble has a better performance than the individual ones. An ensemble is created by averaging (in case of regression) or voting (in case of\n",
    "classification). We will work with **Boosting Enseble method** which turns a weak learning algorithm into a strong learning algorithm. It sequentially create models from a weak learning algorithm and each model tries to eliminate the errors of the previous model. \n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will be using the same dataset as we did in Problem 1. You can either use the same source code to generate data from this problem. The dataset is also provided in the 'data' folder saved as numpy Arrays which can be loaded directly into the program.\n",
    "\n",
    "### A little Recap on the dataset. (Skip if done with Problem 1)\n",
    "\n",
    "The dataset we are using is a 2-D set with points **X** and **y**. We choose the points for **X** as a scaled factor of random number from a uniform distribution ranging from $[0,1)$.\n",
    "$$\n",
    "X = 10*(rand\\in[0,1)) -5\n",
    "$$\n",
    "To generate the **y** points from correspondind **X** points, we define a function **f(x)**.\n",
    "\n",
    "$$\n",
    "f(x)= e^{(-x^2)} + 1.5 e^{-(x-2)^2}\n",
    "$$\n",
    "In this case then, the output points for **y** will be value of **f(x)** plus some noise. The Noise is added by selecting a random number from a Normal Probabilistic Distribution with **zero mean** and $\\sigma$ as standard deviation.\n",
    "\n",
    "$$\n",
    "y = f(x) + N(0,\\sigma)\n",
    "$$\n",
    "where N is a Normal Probabilistic Distribution.\n",
    "Since we wish to evaluate the model, we will split the Training and the Testing set during model fitting and validation. The ratio choosen for the split is 5:1 for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./data/X.npy')\n",
    "y = np.load('./data/y.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(X_train, y_train, \".b\", label=\"Training Data\")\n",
    "plt.plot(X_test, y_test, \".r\", label=\"Test Data\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost with Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core principle of ADABoost is like the above mention Boosting Ensemble Method. It fits to a sequnce of weak learners, those give performance just about the random prediction.\n",
    "\n",
    ">\"The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \n",
    ", \n",
    ", …, \n",
    " to each of the training samples. Initially, those weights are all set to \n",
    ", so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence\"\n",
    "\n",
    "source [ADA Boost Definition at Sklearn](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)\n",
    "\n",
    "Please refer the below image for recalling how the Boosting Method work.\n",
    "\n",
    "<img src='./graphic/1.png' width='550' height='550'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n",
    "\n",
    "We will be using a **Decision Tree Regressor** to fit the *training set* of the datapoints. To show the comparison, we do a similar regression but this time with ADABoost for *n=100* iterations. Once, both the models are trained, we predict the output values for each point in *test dataset* and compare the values of the true and predicted outputs in terms of Mean Squared Error.\n",
    "\n",
    "> \"Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\"\n",
    "\n",
    "source [Decision Tree Definition at Sklearn](https://scikit-learn.org/stable/modules/tree.html#tree)\n",
    "\n",
    "<img src='./graphic/2.png' width='550' height='550'>\n",
    "\n",
    "### Additional reading\n",
    "\n",
    "[Bagging on Low Variance Models](https://towardsdatascience.com/bagging-on-low-variance-models-38d3c70259db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls the random seed given at each base_estimator at each boosting iteration\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# Fit regression model\n",
    "\n",
    "'''Code here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regressor model\n",
    "\n",
    "regr_1.fit(X_train, y_train)\n",
    "regr_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for validation\n",
    "\n",
    "X_test, y_test = zip(*sorted(zip(X_test, y_test)))\n",
    "\n",
    "\n",
    "'''Code here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(X_test, y_test, c=\"k\", label=\"training samples\")\n",
    "plt.plot(X_test, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=3)\n",
    "plt.plot(X_test, y_2, c=\"r\", label=\"n_estimators=100\", linewidth=3)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Boosted Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Now we can use our *test dataset* to predict the output and compute the Mean Squared Error with rest to the true output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse1 = mean_squared_error('''Code here''')\n",
    "print('The MSE for Decision Tree Reg. is : {}'.format(mse1))\n",
    "\n",
    "mse2 = mean_squared_error('''Code here''')\n",
    "print('The MSE for ADABoost Decision Tree Reg. is : {}'.format(mse2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As observed, the mean prediction error in the Boosting Estimator is reduced by half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can you do to further contribute in this notebook ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
